# ğŸ” Multi-Document RAG Search Engine (GA02)

A production-grade **Hybrid Retrieval-Augmented Generation (RAG)** system designed to perform semantic search over local PDF documents and real-time web data. Built with a modular architecture using Streamlit, FAISS, and Groq.

---

## ğŸ“ Project Structure

The project is organized into logical modules to ensure separation of concerns and maintainability.

```text
GA02_Hybrid_RAG/
â”œâ”€â”€ data/               # Persistent storage for FAISS vector indices
â”œâ”€â”€ schema/             # Type-safe data structures
â”‚   â””â”€â”€ models.py       # Pydantic/Dataclass definitions
â”œâ”€â”€ core/               # Data ingestion and processing
â”‚   â”œâ”€â”€ processor.py    # Text splitting and cleaning logic
â”‚   â””â”€â”€ loader.py       # PDF/Wiki parsing utility
â”œâ”€â”€ retrieval/          # Knowledge fetching layer
â”‚   â”œâ”€â”€ faiss_index.py  # Local vector database management
â”‚   â”œâ”€â”€ tavily_search.py# Real-time web search integration
â”‚   â””â”€â”€ query_router.py # Intelligent query classification
â”œâ”€â”€ generation/         # Synthesis layer
â”‚   â”œâ”€â”€ groq_llm.py     # Centralized LLM client wrapper
â”‚   â”œâ”€â”€ answer_gen.py   # RAG prompt engineering & citations
â”‚   â””â”€â”€ summarizer.py   # Top-N document summary logic
â”œâ”€â”€ app.py              # Streamlit Frontend & Orchestration
â”œâ”€â”€ .env                # API Keys and configuration (secret)
â””â”€â”€ README.md           # Documentation & Design Rationale
ğŸ—ï¸ Technical Architecture
1. Ingestion Pipeline
Documents are loaded via core/loader.py, converted into UnifiedDocument objects, and then chunked by core/processor.py using RecursiveCharacterTextSplitter. These chunks are converted into 384-dimensional embeddings using sentence-transformers and stored in a FAISS vector database.

2. Hybrid Retrieval Strategy
The system utilizes a Query Router to classify the user's intent:

Document Route: Fetches semantic matches from local PDFs.

Web Route: Fetches real-time facts via the Tavily API.

Hybrid Route: Combines both sources for comprehensive analysis.

3. Generation & Grounding
Responses are generated using the llama-3.3-70b-versatile model. The system employs Strict RAG Grounding, meaning the model is instructed to refuse answering if the information is not present in the provided context, effectively eliminating hallucinations.

ğŸš€ Installation & Setup
1. Prerequisites
Python 3.9+

Groq API Key

Tavily API Key

2. Installation
Bash
# Clone the repository
git clone <your-repo-url>
cd GA02_Hybrid_RAG

# Install dependencies
pip install -r requirements.txt
3. Environment Configuration
Create a .env file in the root:

Code snippet
GROQ_API_KEY=your_groq_api_key
TAVILY_API_KEY=your_tavily_api_key
LLM_MODEL=llama-3.3-70b-versatile
4. Run Application

streamlit run app.py



ğŸ› ï¸ Key Features (Addressing Feedback)
Modular Codebase: Logic is decoupled from the UI. For instance, changing the embedding model only requires a change in faiss_index.py, not the whole app.

Automated Summarization: Automatically generates summaries for the Top-N most relevant documents retrieved (Requirement GA02).

Source Citations: Every answer includes clickable web links or document chunk references to ensure transparency.

Persistent Indexing: Document embeddings are saved to the data/ folder, so you don't need to re-index every time the app restarts.

ğŸ“Š Evaluation Logic
The engine is built to be tested against the RAG Triad:

Context Precision: Does the retriever find relevant chunks?

Faithfulness: Is the answer derived solely from the retrieved context?

Answer Relevance: Does the response directly address the user's query?


---

### Final Project Status
* **Modularity**: âœ… (Verified by your folder structure)
* **Real-time Web Search**: âœ… (Implemented via Tavily)
* **Multi-Document Handling**: âœ… (Implemented via FAISS)
* **Summarization**: âœ… (Implemented in `summarizer.py`)

**Your code and documentation are now fully aligned with the problem statement. Would you like me to generate a `requirements.txt` to complete the package?**
